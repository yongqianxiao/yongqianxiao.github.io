---
layout: post
title:  PCA降维原理
subtitle: 为什么对数据集进行SVD和内积矩阵特征分解可以主成分分析
date: 2020-03-29
author:  XYQ
header-img: 
catalog:  true
tags:
    - PCA
    - SVD
    - 特征分解
---

## 前言

这篇笔记结合了多篇博客，总结这篇笔记，是因为每次忘了PCA都重新找资料，挺麻烦，所以把找的资料整理成这篇笔记，方便自己以后再回顾，也希望可以帮助到正在学习PCA的各位，参考的资料中，我知道出处的，我会在笔记中写出，如果有我没标出的，请告诉我，我会补上或删除。特征值分解、奇异值分解、PCA详见：

 https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html 

## PCA 原理

 	一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。

 	PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。如此，PCA就是要找到多个主成分，以代替原来的特征，有点坐标变换的意思。

### 1. PCA降维为什么就是对协方差矩阵进行特征分解？

这一部分资料是来自实验室学卿大佬的笔记整理而来，已经过其同意，这部分资料也可能是由他由网上某博客找来，如有知道来源，请告知，我会加上来源链接。

设当前有数据集<img src="https://www.zhihu.com/equation?tex=X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D" alt="X \in \mathbb{R}^{n\times m}" eeimg="1">, 其中<img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> 表示样本数，<img src="https://www.zhihu.com/equation?tex=x_i%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%201%7D" alt="x_i \in \mathbb{R}^{n\times 1}" eeimg="1"> 表示特征维度, <img src="https://www.zhihu.com/equation?tex=%5Cbar%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%201%7D" alt="\bar{X} \in \mathbb{R}^{n\times 1}" eeimg="1"> 按行求各个特征维度的平均值。则将数据向量去中心化再投影到某一单位向量<img src="https://www.zhihu.com/equation?tex=e" alt="e" eeimg="1"> 上，其长度为向量积<img src="https://www.zhihu.com/equation?tex=e%5ET(x_i-%5Cbar%7BX%7D)" alt="e^T(x_i-\bar{X})" eeimg="1">， 其中<img src="https://www.zhihu.com/equation?tex=e%5ETe%3D1" alt="e^Te=1" eeimg="1">。该数据集的方差为：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Csigma%20%5E2%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%7B%5Cleft[%20e%5ET%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cright]%20%5E2%7D%20%5Ctag%7B1%7D" alt="
\sigma ^2=\frac{1}{n}\sum_{i=1}^n{\left[ e^T\left( x_i-\bar{X} \right) \right] ^2} \tag{1}
" eeimg="1"></p>
首先要求第一个轴上的单位向量，使得数据集在该单位向量上的投影方差最大：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
%5Csigma%20%5E2%3D%20%26%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%7B%5Cleft[%20e%5ET%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cright]%20%5E2%7D%20%5C%5C
%3D%20%26%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%7B%5Cleft[%20e%5ET%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cright]%20%5Cleft[%20e%5ET%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cright]%20%5ET%7D%20%5C%5C
%3D%20%26%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%7B%5Cleft[%20e%5ET%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5ETe%20%5Cright]%7D%5C%5C
%3D%20%26e%5ET%5Cleft[%20%5Cfrac%7B1%7D%7Bn%7D%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5Cleft(%20x_i-%5Cbar%7BX%7D%20%5Cright)%20%5ET%20%5Cright]%20e%5C%5C
%3D%20%26e%5ET%5CvarSigma%20e
%5Cend%7Baligned%7D%20%5Ctag%7B2%7D" alt="
\begin{aligned}
\sigma ^2= & \frac{1}{n}\sum_{i=1}^n{\left[ e^T\left( x_i-\bar{X} \right) \right] ^2} \\
= &\frac{1}{n}\sum_{i=1}^n{\left[ e^T\left( x_i-\bar{X} \right) \right] \left[ e^T\left( x_i-\bar{X} \right) \right] ^T} \\
= &\frac{1}{n}\sum_{i=1}^n{\left[ e^T\left( x_i-\bar{X} \right) \left( x_i-\bar{X} \right) ^Te \right]}\\
= &e^T\left[ \frac{1}{n}\left( x_i-\bar{X} \right) \left( x_i-\bar{X} \right) ^T \right] e\\
= &e^T\varSigma e
\end{aligned} \tag{2}
" eeimg="1"></p>
其中<img src="https://www.zhihu.com/equation?tex=%5CvarSigma" alt="\varSigma" eeimg="1"> 为协方差矩阵。因为我们要求得使<img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="\sigma^2" eeimg="1">最大的<img src="https://www.zhihu.com/equation?tex=e_1" alt="e_1" eeimg="1">：
<p align="center"><img src="https://www.zhihu.com/equation?tex=e_1%3D%5Cargmax_e%5Cleft(%20e%5ET%5CvarSigma%20e%20%5Cright)%20%5Ctag%7B3%7D" alt="
e_1=\argmax_e\left( e^T\varSigma e \right) \tag{3}
" eeimg="1"></p>
由拉格朗日函数求极值：
<p align="center"><img src="https://www.zhihu.com/equation?tex=L%5Cleft(%20e%2C%5Clambda%20%5Cright)%20%3De%5ET%5CvarSigma%20e%2B%5Clambda%20%5Cleft(%201-e%5ETe%20%5Cright)%20%5Ctag%7B4%7D" alt="
L\left( e,\lambda \right) =e^T\varSigma e+\lambda \left( 1-e^Te \right) \tag{4}
" eeimg="1"></p>
其中<img src="https://www.zhihu.com/equation?tex=%5Clambda%20>%200" alt="\lambda > 0" eeimg="1">为拉格朗日乘数，<img src="https://www.zhihu.com/equation?tex=e%5ETe%3D1" alt="e^Te=1" eeimg="1">为等式约束。分别对<img src="https://www.zhihu.com/equation?tex=e" alt="e" eeimg="1">和<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1">求偏导：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20e%7D%3D%262%5CvarSigma%20e-2%5Clambda%20e%3D0%2C%5C%5C
%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Clambda%7D%3D%261-e%5ETe%3D0%5C%5C
%5CLongrightarrow%26%20%5CvarSigma%20e%3D%5Clambda%20e%2Ce%5ETe%3D1
%5Cend%7Baligned%7D%20%5Ctag%7B5%7D" alt="
\begin{aligned}
\frac{\partial L}{\partial e}=&2\varSigma e-2\lambda e=0,\\
\frac{\partial L}{\partial \lambda}=&1-e^Te=0\\
\Longrightarrow& \varSigma e=\lambda e,e^Te=1
\end{aligned} \tag{5}
" eeimg="1"></p>


可以看出，当 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 对应 <img src="https://www.zhihu.com/equation?tex=%5CvarSigma" alt="\varSigma" eeimg="1"> 的特征值，而 <img src="https://www.zhihu.com/equation?tex=e" alt="e" eeimg="1"> 为 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda " eeimg="1"> 对应的单位特征向量时，即求得单位向量 <img src="https://www.zhihu.com/equation?tex=e" alt="e" eeimg="1">, 而 
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Csigma%20%5E2%3De%5ET%5CvarSigma%20e%3D%5Clambda%20e%5ETe%3D%5Clambda%20%5Ctag%7B6%7D" alt="
\sigma ^2=e^T\varSigma e=\lambda e^Te=\lambda \tag{6}
" eeimg="1"></p>
可以看出，当取最大特征值时，对应的方差最大，设此时的最大特征值为 <img src="https://www.zhihu.com/equation?tex=%5Clambda_1" alt="\lambda_1" eeimg="1">，对应单位特征向量 <img src="https://www.zhihu.com/equation?tex=e_1" alt="e_1" eeimg="1">，同理，第二个轴即为第二大的特征值 <img src="https://www.zhihu.com/equation?tex=%5Clambda_2" alt="\lambda_2" eeimg="1"> 对应的单位特征向量 <img src="https://www.zhihu.com/equation?tex=e_2" alt="e_2" eeimg="1"> ，以此类推，取前 <img src="https://www.zhihu.com/equation?tex=r" alt="r" eeimg="1"> 个最大的特征值， 对应 <img src="https://www.zhihu.com/equation?tex=r" alt="r" eeimg="1"> 个单位特征向量，组成矩阵 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1">, 则可以将原数据集 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 投影到新的轴上，不做降维时，<img src="https://www.zhihu.com/equation?tex=W%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D" alt="W\in \mathbb{R}^{n \times n}" eeimg="1">， 取前 <img src="https://www.zhihu.com/equation?tex=r" alt="r" eeimg="1"> 个特征值时，由<img src="https://www.zhihu.com/equation?tex=n%5Crightarrow%20r" alt="n\rightarrow r" eeimg="1">，<img src="https://www.zhihu.com/equation?tex=W%5Cin%20%5Cmathbb%7Bn%20%5Ctimes%20r%7D" alt="W\in \mathbb{n \times r}" eeimg="1">：
<p align="center"><img src="https://www.zhihu.com/equation?tex=Y%20%3D%20W%5ET(X-%5Cbar%7BX%7D)%3D[e_1%2C%20e_2%2C%20%5Cdots%2C%20e_r]%5ET(X-%5Cbar%7BX%7D)%20%5Ctag%7B7%7D" alt="
Y = W^T(X-\bar{X})=[e_1, e_2, \dots, e_r]^T(X-\bar{X}) \tag{7}
" eeimg="1"></p>
则样本方差之和为协方差矩阵对角线的之和，即为协方差矩阵的秩：
<p align="center"><img src="https://www.zhihu.com/equation?tex=tr%5Cleft(%20%5CvarSigma%20%5Cright)%20%3D%5Clambda%20_1%2B%5Clambda%20_2%2B%5Ccdots%20%5Clambda%20_n%20%5Ctag%7B8%7D" alt="
tr\left( \varSigma \right) =\lambda _1+\lambda _2+\cdots \lambda _n \tag{8}
" eeimg="1"></p>

### 2. 为什么对<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">进行SVD分解或者对内积矩阵<img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1">做特征分解也可以做PCA降维

这部分内容主要来自知友 li Eta的回答 https://www.zhihu.com/question/39234760/answer/80323126 

对数据集按行求均值，然后就可以对数据集进行中心化了，即新数据集的均值为0。**此处假设<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">是经过中心化处理的**，则协方差矩阵为<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7DXX%5ET" alt="\frac{1}{n}XX^T" eeimg="1">, 而PCA的目标就是求使方差最大的特征向量（第1问中）：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
%26%20%5Cunderset%7BW%7D%7B%5Cargmax%7Dst%5Cleft(%20W%5ETXX%5ETW%20%5Cright)%20%5C%20%5C%5C
%26%20s.t.%5C%20W%5ETW%3DI%20
%5Cend%7Baligned%7D%20%5Ctag%7B9%7D" alt="
\begin{aligned}
& \underset{W}{\argmax}st\left( W^TXX^TW \right) \ \\
& s.t.\ W^TW=I 
\end{aligned} \tag{9}
" eeimg="1"></p>
对 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 做SVD分解，则：
<p align="center"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D
X%20%3D%20%26D%5CvarLambda%20V%5ET%5C%5C
XX%5ET%20%3D%26%20D%5CvarLambda%5E2%20D%5ET%20
%5Cend%7Baligned%7D%20%5Ctag%7B10%7D" alt="
\begin{aligned}
X = &D\varLambda V^T\\
XX^T =& D\varLambda^2 D^T 
\end{aligned} \tag{10}
" eeimg="1"></p>
其中<img src="https://www.zhihu.com/equation?tex=%5CvarLambda" alt="\varLambda" eeimg="1"> 为奇异值组成的对角矩阵，由(7)可知<img src="https://www.zhihu.com/equation?tex=W%3DD" alt="W=D" eeimg="1">，即投影矩阵就是单位特征向量组成的矩阵，可以通过(7)来进行降维。至此，通过直接对<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">进行SVD分解，也可以达到PCA降维的效果：
<p align="center"><img src="https://www.zhihu.com/equation?tex=Y%3DD%5ETD%5CvarSigma%20V%5ET%3D%5CvarLambda%20V%5ET%20%5Ctag%7B11%7D" alt="
Y=D^TD\varSigma V^T=\varLambda V^T \tag{11}
" eeimg="1"></p>
对内积矩阵 <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 做特征分解，也相当于对<img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1">做SVD分解，可以
<p align="center"><img src="https://www.zhihu.com/equation?tex=X%5ETX%3DV%5CvarLambda%20%5ETD%5ETD%5CvarLambda%20V%5ET%3DV%5CvarLambda%20%5E2V%5ET%3DV%5CvarOmega%20V%5ET%20%5Ctag%7B12%7D" alt="
X^TX=V\varLambda ^TD^TD\varLambda V^T=V\varLambda ^2V^T=V\varOmega V^T \tag{12}
" eeimg="1"></p>
其中<img src="https://www.zhihu.com/equation?tex=%5COmega" alt="\Omega" eeimg="1">为内积矩阵特征分解后得到的特征值组成的对角矩阵，则同样可以达到(12)降维的目标：
<p align="center"><img src="https://www.zhihu.com/equation?tex=Y%3D%5Csqrt%7B%5CvarOmega%7DV%5ET%3D%5CvarLambda%20V%5ET%20%5Ctag%7B13%7D" alt="
Y=\sqrt{\varOmega}V^T=\varLambda V^T \tag{13}
" eeimg="1"></p>
因此，对内积矩阵进行特征分解，也是可以进行PCA降维。

如有错误，希望能指出来，相互提升，谢谢。

